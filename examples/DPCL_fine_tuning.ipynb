{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with 40 male and female speakers\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import H5PY_RW\n",
    "from data.data_tools import read_metadata, males_keys, females_keys\n",
    "\n",
    "H5_dic = read_metadata()\n",
    "chunk_size = 512*10\n",
    "\n",
    "males = H5PY_RW('test_raw_16k.h5py', subset = males_keys(H5_dic)).set_chunk(chunk_size).shuffle()\n",
    "fem = H5PY_RW('test_raw_16k.h5py', subset = females_keys(H5_dic)).set_chunk(chunk_size).shuffle()\n",
    "print 'Data with', len(H5_dic), 'male and female speakers'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import Mixer\n",
    "\n",
    "mixed_data = Mixer([males, fem], with_mask=False, with_inputs=True)\n",
    "\n",
    "# Training set selection\n",
    "mixed_data.select_split(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model pretrained loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "max_pool = 128\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "config_model = {}\n",
    "config_model[\"type\"] = \"DPCL_train_front\"\n",
    "\n",
    "config_model[\"batch_size\"] = batch_size\n",
    "config_model[\"chunk_size\"] = chunk_size\n",
    "\n",
    "config_model[\"N\"] = N\n",
    "config_model[\"maxpool\"] = max_pool\n",
    "config_model[\"window\"] = 1024\n",
    "\n",
    "config_model[\"smooth_size\"] = 20\n",
    "\n",
    "config_model[\"alpha\"] = learning_rate\n",
    "config_model[\"reg\"] = 1e-4\n",
    "config_model[\"beta\"] = 0.1\n",
    "config_model[\"rho\"] = 0.01\n",
    "\n",
    "idd = ''.join('-{}={}-'.format(key, val) for key, val in sorted(config_model.items()))\n",
    "config_model[\"type\"] = \"DPCL_finetuning\"\n",
    "config_model[\"batch_size\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID : AdaptiveNet-crimson-frog-5697\n",
      "INFO:tensorflow:Restoring parameters from /home/anthony/das/log/DPCL_train_front/AdaptiveNet-wispy-forest-8617-N=256--alpha=0.001--batch_size=8--beta=0.1--chunk_size=5120--maxpool=128--reg=0.0001--rho=0.01--smooth_size=20--type=DPCL_train_front--window=1024-/model.ckpt\n",
      "Tensor(\"separate/Reshape_5:0\", shape=(?, ?, 256, 1), dtype=float32)\n",
      "[<tf.Variable 'conv/W:0' shape=(1, 1024, 1, 256) dtype=float32_ref>, <tf.Variable 'smooth/smoothing_filter:0' shape=(1, 20, 256, 256) dtype=float32_ref>, <tf.Variable 'prediction/W:0' shape=(1, 600, 10240) dtype=float32_ref>, <tf.Variable 'prediction/b:0' shape=(10240,) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_1/rnn/basic_lstm_cell/kernel:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_1/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_1/rnn/basic_lstm_cell/kernel:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_1/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_2/rnn/basic_lstm_cell/kernel:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_2/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_2/rnn/basic_lstm_cell/kernel:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_2/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "from models.adapt import Adapt\n",
    "import config\n",
    "full_id = 'wispy-forest-8617'+idd\n",
    "\n",
    "folder='DPCL_fine_tuning'\n",
    "model = Adapt(config_model=config_model,pretraining= False, folder=folder)\n",
    "path = os.path.join(config.workdir, 'log', 'DPCL_train_front')\n",
    "\n",
    "from models.dpcl import DPCL\n",
    "\n",
    "with model.graph.as_default():\n",
    "    model.connect_front(DPCL)\n",
    "    model.sepNet.output = model.sepNet.prediction\n",
    "    model.create_saver()\n",
    "    model.restore_model(path, full_id)\n",
    "    model.sepNet.output = model.sepNet.separate\n",
    "    model.separator\n",
    "    model.back\n",
    "    model.cost\n",
    "    model.optimize\n",
    "    model.tensorboard_init()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect DPCL model to the front end and back end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'optimize/conv/W/RMSProp:0' shape=(1, 1024, 1, 256) dtype=float32_ref>, <tf.Variable 'optimize/conv/W/RMSProp_1:0' shape=(1, 1024, 1, 256) dtype=float32_ref>, <tf.Variable 'optimize/smooth/smoothing_filter/RMSProp:0' shape=(1, 20, 256, 256) dtype=float32_ref>, <tf.Variable 'optimize/smooth/smoothing_filter/RMSProp_1:0' shape=(1, 20, 256, 256) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "from itertools import compress\n",
    "with model.graph.as_default():\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = model.sess.run([~(tf.is_variable_initialized(var)) \\\n",
    "                                   for var in global_vars])\n",
    "    not_initialized_vars = list(compress(global_vars, is_not_initialized))\n",
    "    print not_initialized_vars\n",
    "    if len(not_initialized_vars):\n",
    "        init = tf.variables_initializer(not_initialized_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0  loss= 1.1536\n",
      "DAS model saved at iteration number  0  with cost =  1.1536\n",
      "Step # 1  loss= 0.927743\n",
      "Step # 2  loss= 0.931067\n",
      "Step # 3  loss= 0.227723\n",
      "Step # 4  loss= 0.972699\n",
      "Step # 5  loss= 1.09358\n",
      "Step # 6  loss= 0.594789\n",
      "Step # 7  loss= 1.2961\n",
      "Step # 8  loss= 1.20904\n",
      "Step # 9  loss= 0.615677\n",
      "Step # 10  loss= 0.719588\n",
      "Step # 11  loss= 0.660654\n",
      "Step # 12  loss= 0.915523\n",
      "Step # 13  loss= 0.61194\n",
      "Step # 14  loss= 0.893956\n",
      "Step # 15  loss= 1.06256\n",
      "Step # 16  loss= 0.578016\n",
      "Step # 17  loss= 0.622088\n",
      "Step # 18  loss= 0.698267\n",
      "Step # 19  loss= 0.731227\n",
      "Step # 20  loss= 0.855751\n",
      "DAS model saved at iteration number  20  with cost =  0.855751\n",
      "Step # 21  loss= 0.566559\n",
      "Step # 22  loss= 0.781461\n",
      "Step # 23  loss= 0.678661\n",
      "Step # 24  loss= 0.805829\n",
      "Step # 25  loss= 0.822999\n",
      "Step # 26  loss= 1.16017\n",
      "Step # 27  loss= 0.721024\n",
      "Step # 28  loss= 0.831858\n",
      "Step # 29  loss= 0.623625\n",
      "Step # 30  loss= 1.07433\n",
      "Step # 31  loss= 0.584814\n",
      "Step # 32  loss= 0.80182\n",
      "Step # 33  loss= 0.589364\n",
      "Step # 34  loss= 0.765553\n",
      "Step # 35  loss= 0.490958\n",
      "Step # 36  loss= 0.827363\n",
      "Step # 37  loss= 0.353245\n",
      "Step # 38  loss= 0.578742\n",
      "Step # 39  loss= 0.791287\n",
      "Step # 40  loss= 0.674701\n",
      "DAS model saved at iteration number  40  with cost =  0.674701\n",
      "Step # 41  loss= 0.998176\n",
      "Step # 42  loss= 0.717924\n",
      "Step # 43  loss= 0.513739\n",
      "Step # 44  loss= 0.661418\n",
      "Step # 45  loss= 0.487178\n",
      "Step # 46  loss= 0.6065\n",
      "Step # 47  loss= 5.93971\n",
      "Step # 48  loss= 0.577697\n",
      "Step # 49  loss= 0.580401\n",
      "Step # 50  loss= 0.882434\n",
      "Step # 51  loss= 0.45776\n",
      "Step # 52  loss= 1.05441\n",
      "Step # 53  loss= 1.27206\n",
      "Step # 54  loss= 1.3535\n",
      "Step # 55  loss= 0.990492\n",
      "Step # 56  loss= 0.731438\n",
      "Step # 57  loss= 0.849169\n",
      "Step # 58  loss= 0.541332\n",
      "Step # 59  loss= 0.51685\n",
      "Step # 60  loss= 0.698234\n",
      "DAS model saved at iteration number  60  with cost =  0.698234\n",
      "Step # 61  loss= 0.984017\n",
      "Step # 62  loss= 0.74649\n",
      "Step # 63  loss= 0.562732\n",
      "Step # 64  loss= 0.741325\n",
      "Step # 65  loss= 1.03096\n",
      "Step # 66  loss= 11.7922\n",
      "Step # 67  loss= 1.03626\n",
      "Step # 68  loss= 0.9296\n",
      "Step # 69  loss= 0.78321\n",
      "Step # 70  loss= 0.825997\n",
      "Step # 71  loss= 0.686728\n",
      "Step # 72  loss= 1.03881\n",
      "Step # 73  loss= 0.995214\n",
      "Step # 74  loss= 0.811763\n",
      "Step # 75  loss= 1.03448\n",
      "Step # 76  loss= 0.977118\n",
      "Step # 77  loss= 0.611305\n",
      "Step # 78  loss= 0.902746\n",
      "Step # 79  loss= 0.967507\n",
      "Step # 80  loss= 1.06663\n",
      "DAS model saved at iteration number  80  with cost =  1.06663\n",
      "Step # 81  loss= 0.565755\n",
      "Step # 82  loss= 1.07978\n",
      "Step # 83  loss= 1.01909\n",
      "Step # 84  loss= 0.827723\n",
      "Step # 85  loss= 0.693435\n",
      "Step # 86  loss= 0.961068\n",
      "Step # 87  loss= 0.829732\n",
      "Step # 88  loss= 0.923218\n",
      "Step # 89  loss= 0.883265\n",
      "Step # 90  loss= 0.615044\n",
      "Step # 91  loss= 0.647269\n",
      "Step # 92  loss= 0.941056\n",
      "Step # 93  loss= 0.51964\n",
      "Step # 94  loss= 0.712446\n",
      "Step # 95  loss= 0.541361\n",
      "Step # 96  loss= 0.700153\n",
      "Step # 97  loss= 0.740735\n",
      "Step # 98  loss= 0.697703\n",
      "Step # 99  loss= 1.03164\n",
      "Step # 100  loss= 0.601099\n",
      "DAS model saved at iteration number  100  with cost =  0.601099\n",
      "Step # 101  loss= 0.50209\n",
      "Step # 102  loss= 0.71543\n",
      "Step # 103  loss= 1.13186\n",
      "Step # 104  loss= 0.666611\n",
      "Step # 105  loss= 0.861554\n",
      "Step # 106  loss= 0.445301\n",
      "Step # 107  loss= 0.747651\n",
      "Step # 108  loss= 1.03891\n",
      "Step # 109  loss= 0.505486\n",
      "Step # 110  loss= 0.525964\n",
      "Step # 111  loss= 0.696164\n",
      "Step # 112  loss= 1.07266\n",
      "Step # 113  loss= 1.18904\n",
      "Step # 114  loss= 0.811492\n",
      "Step # 115  loss= 0.8227\n",
      "Step # 116  loss= 0.878609\n",
      "Step # 117  loss= 0.650334\n",
      "Step # 118  loss= 0.476219\n",
      "Step # 119  loss= 0.641786\n",
      "Step # 120  loss= 0.608642\n",
      "DAS model saved at iteration number  120  with cost =  0.608642\n",
      "Step # 121  loss= 0.875364\n",
      "Step # 122  loss= 0.618449\n",
      "Step # 123  loss= 53.1977\n",
      "Step # 124  loss= 21063.4\n",
      "Step # 125  loss= 1.85696e+07\n",
      "Step # 126  loss= 1.54107e+20\n",
      "Step # 127  loss= 8.20478e+14\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: conv_1/summaries/histogram\n\t [[Node: conv_1/summaries/histogram = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv_1/summaries/histogram/tag, conv/W/read/_139)]]\n\nCaused by op u'conv_1/summaries/histogram', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-acb6ea0f41ea>\", line 21, in <module>\n    model.tensorboard_init()\n  File \"/home/anthony/das/models/adapt.py\", line 114, in tensorboard_init\n    variable_summaries(self.W)\n  File \"/home/anthony/das/utils/ops/ops.py\", line 39, in variable_summaries\n    tf.summary.histogram('histogram', var)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: conv_1/summaries/histogram\n\t [[Node: conv_1/summaries/histogram = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv_1/summaries/histogram/tag, conv/W/read/_139)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4267d3685a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Step #'\u001b[0m  \u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' loss='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anthony/das/models/adapt.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_mix, X_in, learning_rate, step, ind_train)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mind_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                         \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_mix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_non_mix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_mix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_non_mix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mind_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: conv_1/summaries/histogram\n\t [[Node: conv_1/summaries/histogram = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv_1/summaries/histogram/tag, conv/W/read/_139)]]\n\nCaused by op u'conv_1/summaries/histogram', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-acb6ea0f41ea>\", line 21, in <module>\n    model.tensorboard_init()\n  File \"/home/anthony/das/models/adapt.py\", line 114, in tensorboard_init\n    variable_summaries(self.W)\n  File \"/home/anthony/das/utils/ops/ops.py\", line 39, in variable_summaries\n    tf.summary.histogram('histogram', var)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: conv_1/summaries/histogram\n\t [[Node: conv_1/summaries/histogram = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv_1/summaries/histogram/tag, conv/W/read/_139)]]\n"
     ]
    }
   ],
   "source": [
    "nb_iterations = 1000\n",
    "#initialize the model\n",
    "model.sess.run(init)\n",
    "\n",
    "for i in range(nb_iterations):\n",
    "    X_in, X_mix, Ind = mixed_data.get_batch(batch_size)\n",
    "    c = model.train(X_mix, X_in, learning_rate, i)\n",
    "    print 'Step #'  ,i,' loss=', c \n",
    "\n",
    "    if i%20 == 0: #cost_valid < cost_valid_min:\n",
    "        print 'DAS model saved at iteration number ', i,' with cost = ', c \n",
    "        model.save(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
