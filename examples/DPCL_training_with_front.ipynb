{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with 40 male and female speakers\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import H5PY_RW\n",
    "from data.data_tools import read_data_header, males_keys, females_keys\n",
    "\n",
    "H5_dic = read_data_header()\n",
    "chunk_size = 512*30\n",
    "\n",
    "males = H5PY_RW().open_h5_dataset('test_raw.h5py', subset = males_keys(H5_dic)).set_chunk(chunk_size).shuffle()\n",
    "fem = H5PY_RW().open_h5_dataset('test_raw.h5py', subset = females_keys(H5_dic)).set_chunk(chunk_size).shuffle()\n",
    "print 'Data with', len(H5_dic), 'male and female speakers'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import Mixer\n",
    "\n",
    "mixed_data = Mixer([males, fem], with_mask=False, with_inputs=True)\n",
    "\n",
    "# Training set selection\n",
    "mixed_data.select_split(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model pretrained loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 1\n",
    "N = 256\n",
    "max_pool = 256\n",
    "regularization = 0.0001\n",
    "\n",
    "config_model = {}\n",
    "config_model[\"alpha\"] = learning_rate\n",
    "config_model[\"reg\"] = regularization\n",
    "config_model[\"batch_size\"] = batch_size\n",
    "config_model[\"chunk_size\"] = chunk_size\n",
    "config_model[\"N\"] = N\n",
    "config_model[\"maxpool\"] = max_pool\n",
    "config_model[\"type\"] = \"pretraining\"\n",
    "config_model[\"smooth_size\"] = 10\n",
    "idd = ''.join('-{}={}-'.format(key, val) for key, val in sorted(config_model.items()))\n",
    "config_model[\"type\"] = \"DPCL_train_front\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID : AdaptiveNet-shiny-frog-0617\n",
      "INFO:tensorflow:Restoring parameters from /home/anthony/das/log/pretraining/AdaptiveNet-flat-limit-0477-N=256--alpha=0.0001--batch_size=1--chunk_size=15360--maxpool=256--reg=0.0001--smooth_size=10--type=pretraining-/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from models.adapt import Adapt\n",
    "\n",
    "full_id = 'flat-limit-0477'+idd\n",
    "\n",
    "folder='DPCL_train_front'\n",
    "model = Adapt(config_model=config_model,pretraining= False, folder=folder)\n",
    "model.create_saver()\n",
    "model.restore_model('pretraining', full_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect DPCL model to the front end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"prediction/l2_normalize:0\", shape=(?, ?, 256, 40), dtype=float32)\n",
      "[<tf.Variable 'conv/W:0' shape=(1, 1024, 1, 256) dtype=float32_ref>, <tf.Variable 'smooth/smoothing_filter:0' shape=(1, 10, 1, 1) dtype=float32_ref>, <tf.Variable 'prediction/W:0' shape=(1, 600, 10240) dtype=float32_ref>, <tf.Variable 'prediction/b:0' shape=(10240,) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_1/rnn/basic_lstm_cell/kernel:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_1/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_1/rnn/basic_lstm_cell/kernel:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_1/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_2/rnn/basic_lstm_cell/kernel:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_2/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_2/rnn/basic_lstm_cell/kernel:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_2/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "from models.dpcl import DPCL\n",
    "\n",
    "with model.graph.as_default():\n",
    "    model.connect_front(DPCL)\n",
    "    model.sepNet.output = model.sepNet.prediction\n",
    "    model.cost = model.sepNet.cost\n",
    "    model.optimize\n",
    "    model.freeze_front()\n",
    "    model.tensorboard_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'prediction/W:0' shape=(1, 600, 10240) dtype=float32_ref>, <tf.Variable 'prediction/b:0' shape=(10240,) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_1/rnn/basic_lstm_cell/kernel:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_1/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_1/rnn/basic_lstm_cell/kernel:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_1/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_2/rnn/basic_lstm_cell/kernel:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'prediction/forward_BLSTM_2/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_2/rnn/basic_lstm_cell/kernel:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'prediction/backward_BLSTM_2/rnn/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'optimize/beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'optimize/conv/W/Adam:0' shape=(1, 1024, 1, 256) dtype=float32_ref>, <tf.Variable 'optimize/conv/W/Adam_1:0' shape=(1, 1024, 1, 256) dtype=float32_ref>, <tf.Variable 'optimize/smooth/smoothing_filter/Adam:0' shape=(1, 10, 1, 1) dtype=float32_ref>, <tf.Variable 'optimize/smooth/smoothing_filter/Adam_1:0' shape=(1, 10, 1, 1) dtype=float32_ref>, <tf.Variable 'optimize/prediction/W/Adam:0' shape=(1, 600, 10240) dtype=float32_ref>, <tf.Variable 'optimize/prediction/W/Adam_1:0' shape=(1, 600, 10240) dtype=float32_ref>, <tf.Variable 'optimize/prediction/b/Adam:0' shape=(10240,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/b/Adam_1:0' shape=(10240,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_1/rnn/basic_lstm_cell/kernel/Adam:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_1/rnn/basic_lstm_cell/kernel/Adam_1:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_1/rnn/basic_lstm_cell/bias/Adam:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_1/rnn/basic_lstm_cell/bias/Adam_1:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_1/rnn/basic_lstm_cell/kernel/Adam:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_1/rnn/basic_lstm_cell/kernel/Adam_1:0' shape=(556, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_1/rnn/basic_lstm_cell/bias/Adam:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_1/rnn/basic_lstm_cell/bias/Adam_1:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_2/rnn/basic_lstm_cell/kernel/Adam:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_2/rnn/basic_lstm_cell/kernel/Adam_1:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_2/rnn/basic_lstm_cell/bias/Adam:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/forward_BLSTM_2/rnn/basic_lstm_cell/bias/Adam_1:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_2/rnn/basic_lstm_cell/kernel/Adam:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_2/rnn/basic_lstm_cell/kernel/Adam_1:0' shape=(900, 1200) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_2/rnn/basic_lstm_cell/bias/Adam:0' shape=(1200,) dtype=float32_ref>, <tf.Variable 'optimize/prediction/backward_BLSTM_2/rnn/basic_lstm_cell/bias/Adam_1:0' shape=(1200,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "with model.graph.as_default():\n",
    "    uninit =[v for v in tf.global_variables() if v.name.split(':')[0] in set(model.sess.run(tf.report_uninitialized_variables()))]\n",
    "    print uninit\n",
    "    init = tf.variables_initializer(uninit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0  loss= 137.988\n",
      "DAS model saved at iteration number  0  with cost =  137.988\n",
      "Step # 1  loss= 127.968\n",
      "Step # 2  loss= 119.8\n",
      "Step # 3  loss= 114.804\n",
      "Step # 4  loss= 110.379\n",
      "Step # 5  loss= 107.097\n",
      "Step # 6  loss= 104.243\n",
      "Step # 7  loss= 100.086\n",
      "Step # 8  loss= 100.534\n",
      "Step # 9  loss= 98.9576\n",
      "Step # 10  loss= 96.7935\n",
      "Step # 11  loss= 96.4269\n",
      "Step # 12  loss= 95.1378\n",
      "Step # 13  loss= 94.222\n",
      "Step # 14  loss= 93.127\n",
      "Step # 15  loss= 92.7828\n",
      "Step # 16  loss= 92.2283\n",
      "Step # 17  loss= 91.6716\n",
      "Step # 18  loss= 90.722\n",
      "Step # 19  loss= 90.8449\n",
      "Step # 20  loss= 90.4047\n",
      "DAS model saved at iteration number  20  with cost =  90.4047\n",
      "Step # 21  loss= 89.5077\n",
      "Step # 22  loss= 89.4204\n",
      "Step # 23  loss= 89.4762\n",
      "Step # 24  loss= 89.2782\n",
      "Step # 25  loss= 88.8426\n",
      "Step # 26  loss= 87.8713\n",
      "Step # 27  loss= 88.3852\n",
      "Step # 28  loss= 88.4611\n",
      "Step # 29  loss= 88.1511\n",
      "Step # 30  loss= 87.9636\n",
      "Step # 31  loss= 87.4099\n",
      "Step # 32  loss= 84.8185\n",
      "Step # 33  loss= 87.5965\n",
      "Step # 34  loss= 86.9726\n",
      "Step # 35  loss= 87.0923\n",
      "Step # 36  loss= 87.2141\n",
      "Step # 37  loss= 87.06\n",
      "Step # 38  loss= 86.8983\n",
      "Step # 39  loss= 86.6861\n",
      "Step # 40  loss= 84.3986\n",
      "DAS model saved at iteration number  40  with cost =  84.3986\n",
      "Step # 41  loss= 86.5536\n",
      "Step # 42  loss= 86.434\n",
      "Step # 43  loss= 85.414\n",
      "Step # 44  loss= 86.2354\n",
      "Step # 45  loss= 85.954\n",
      "Step # 46  loss= 85.9533\n",
      "Step # 47  loss= 85.9327\n",
      "Step # 48  loss= 85.7105\n",
      "Step # 49  loss= 85.5691\n",
      "Step # 50  loss= 85.5389\n",
      "Step # 51  loss= 85.4572\n",
      "Step # 52  loss= 85.3239\n",
      "Step # 53  loss= 85.3148\n",
      "Step # 54  loss= 85.0027\n",
      "Step # 55  loss= 84.6897\n",
      "Step # 56  loss= 84.9888\n",
      "Step # 57  loss= 84.9128\n",
      "Step # 58  loss= 84.7764\n",
      "Step # 59  loss= 84.495\n",
      "Step # 60  loss= 84.6664\n",
      "DAS model saved at iteration number  60  with cost =  84.6664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-98a0b1ffb159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#cost_valid < cost_valid_min:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'DAS model saved at iteration number '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' with cost = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anthony/das/models/adapt.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1494\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m         clear_extraneous_savers=clear_extraneous_savers)\n\u001b[0m\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[1;32m   1765\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1768\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[0;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope, exclude_nodes, clear_extraneous_savers)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m   \u001b[0;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_iterations = 1000\n",
    "\n",
    "#initialize the model\n",
    "model.sess.run(init)\n",
    "\n",
    "for i in range(nb_iterations):\n",
    "    X_in, X_mix, Ind = mixed_data.get_batch(batch_size)\n",
    "    c = model.train(X_mix, X_in, learning_rate, i)\n",
    "    print 'Step #'  ,i,' loss=', c \n",
    "\n",
    "    if i%20 == 0: #cost_valid < cost_valid_min:\n",
    "        print 'DAS model saved at iteration number ', i,' with cost = ', c \n",
    "        model.save(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
